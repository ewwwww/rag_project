from langchain_community.llms import Tongyi
from data_process import load_knowledge_base
from langchain_community.embeddings import DashScopeEmbeddings
import os

# è®¾ç½®æŸ¥è¯¢é—®é¢˜
# query = "å®¢æˆ·ç»ç†è¢«æŠ•è¯‰äº†ï¼ŒæŠ•è¯‰ä¸€æ¬¡æ‰£å¤šå°‘åˆ†ï¼Ÿ"

def run_query_mode(query: str, vector_store_path: str = "./vector_store"):
    """
    è¿è¡ŒæŸ¥è¯¢æ¨¡å¼ï¼šä½¿ç”¨å·²åˆå§‹åŒ–çš„çŸ¥è¯†åº“è¿›è¡Œé—®ç­”
    
    å‚æ•°:
        query: ç”¨æˆ·æŸ¥è¯¢é—®é¢˜
        vector_store_path: å‘é‡æ•°æ®åº“è·¯å¾„ï¼ˆé»˜è®¤ä½¿ç”¨ ./vector_storeï¼‰
    
    è¿”å›:
        bool: æŸ¥è¯¢æ˜¯å¦æˆåŠŸæ‰§è¡Œ
    """
    try:
        print(f"\næ­£åœ¨å¤„ç†æŸ¥è¯¢ï¼š{query}")
        print("-" * 50)
        user_query(query, vector_store_path)
        print("-" * 50)
        return True
    except Exception as e:
        print(f"âŒ æŸ¥è¯¢å¤„ç†å¤±è´¥ï¼š{e}")
        return False


def user_query(query: str, vector_store_path: str = "./vector_store"):
    if query:
        # ç¤ºä¾‹ï¼šå¦‚ä½•åŠ è½½å·²ä¿å­˜çš„å‘é‡æ•°æ®åº“
        # æ³¨é‡Šæ‰ä»¥ä¸‹ä»£ç ä»¥é¿å…åœ¨å½“å‰è¿è¡Œä¸­é‡å¤åŠ è½½
        # åˆ›å»ºåµŒå…¥æ¨¡å‹
        embeddings = DashScopeEmbeddings(
            model="text-embedding-v2"
        )
        # ä»ç£ç›˜åŠ è½½å‘é‡æ•°æ®åº“
        loaded_knowledgeBase = load_knowledge_base(vector_store_path, embeddings)
        # ä½¿ç”¨åŠ è½½çš„çŸ¥è¯†åº“è¿›è¡ŒæŸ¥è¯¢
        docs = loaded_knowledgeBase.similarity_search(query)
        
        # åˆå§‹åŒ–å¯¹è¯å¤§æ¨¡å‹
        DASHSCOPE_API_KEY = os.getenv("DASHSCOPE_API_KEY")
        llm = Tongyi(model_name="deepseek-v3", dashscope_api_key=DASHSCOPE_API_KEY)
        
        # ä½¿ç”¨ç®€å•çš„ LLM è°ƒç”¨æ¨¡å¼ï¼ˆå…¼å®¹æ‰€æœ‰ç‰ˆæœ¬ï¼‰
        # å°†æ–‡æ¡£å†…å®¹ç»„åˆä½œä¸ºä¸Šä¸‹æ–‡
        context = "\n\n".join([doc.page_content for doc in docs])
        prompt = f"åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ï¼š\n\n{context}\n\né—®é¢˜ï¼š{query}\n\nç­”æ¡ˆï¼š"
        

        response_text = llm.invoke(prompt)

        
        print("æŸ¥è¯¢å·²å¤„ç†ã€‚")
        print(response_text)
        print("\n" + "=" * 50)
        print("ğŸ“š ç­”æ¡ˆæ¥æº:")
        print("=" * 50)

        # è®°å½•å”¯ä¸€çš„æ¥æºä¿¡æ¯ï¼ˆPDFåç§°å’Œé¡µç ï¼‰
        unique_sources = set()

        # æ˜¾ç¤ºæ¯ä¸ªæ–‡æ¡£å—çš„æ¥æºä¿¡æ¯
        for doc in docs:
            text_content = getattr(doc, "page_content", "")
            source_info = loaded_knowledgeBase.page_info.get(
                text_content.strip(), "æœªçŸ¥"
            )

            if source_info not in unique_sources:
                unique_sources.add(source_info)
                # è§£æPDFåç§°å’Œé¡µç 
                if ":" in str(source_info):
                    pdf_name, page_num = str(source_info).split(":", 1)
                    print(f"  ğŸ“„ æ–‡æ¡£: {pdf_name}")
                    print(f"  ğŸ“‘ é¡µç : ç¬¬ {page_num} é¡µ")
                    print()
                else:
                    # å…¼å®¹æ—§æ ¼å¼ï¼ˆçº¯æ•°å­—é¡µç ï¼‰
                    print(f"  ğŸ“‘ é¡µç : ç¬¬ {source_info} é¡µ")
                    print()